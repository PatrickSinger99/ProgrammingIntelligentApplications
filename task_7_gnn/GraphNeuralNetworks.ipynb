{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNN)\n",
    "\n",
    "Graph based deep learning is currently one of the hottest topics in Machine Learning Research. In the NeurIPS 2020 conference GNNs constituted the most prominent topic, as can be seen in this [list of conference papers](https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_neurips20/README.md). \n",
    "\n",
    "However, GNNs are not only subject of research. They have already found their way into a wide range of [applications](https://medium.com/criteo-engineering/top-applications-of-graph-neural-networks-2021-c06ec82bfc18).\n",
    "\n",
    "Graph Neural Networks are suitable for Machine Learning tasks on data with structural \n",
    "relation between the individual data-points. Examples are e.g. social and communication networks analysis, traffic prediction, fraud detection and the classification tasks in this exercise. [Graph Representation Learning](https://www.cs.mcgill.ca/~wlh/grl_book/)\n",
    "aims to build and train models for graph datasets to be used for a variety of ML tasks.\n",
    "\n",
    "The goals of this lecture are:\n",
    "* Understand the basic concepts of Graph Neural Networks and their application categories\n",
    "* Understand how to create a custom dataset for Graph Neural Networks\n",
    "* Learn to implement a Graph Convolutional Network (GCN) for Node Classification by using the [Spektral](https://graphneural.network/)-framework\n",
    "* Apply a Graph Convolutional Network to predict the genre of a song. For this a comprehensive set of playlists and tracks is accessed from **spotify** via the Python API [spotipy](https://spotipy.readthedocs.io/en/2.18.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course of Action\n",
    "\n",
    "* Please write all executable python code in ```Code```-Cells (```Cell```->```Cell Type```->```Code```) and all Text as [Markdown](http://commonmark.org/help/) in ```Markdown```-Cells\n",
    "* Describe your thinking and your decisions (where appropriate) in an extra Markdown Cell or via Python comments\n",
    "* In general: discuss all your results and comment on them (are they good/bad/unexpected, could they be improved, how?, etc.). Furthermore, visualise your data (input and output).\n",
    "* Write a short general conclusion at the end of the notebook\n",
    "* Further experiments are encouraged. However, don't forget to comment on your reasoning.\n",
    "* Use a scientific approach for all experiments (i.e. develop a hypothesis or concrete question, make observations, evaluate results)\n",
    "\n",
    "## Submission\n",
    "\n",
    "Upload your complete Notebook to [Ilias](https://learn.mi.hdm-stuttgart.de/ilias/ilias.php?ref_id=21049&cmdClass=ilrepositorygui&cmdNode=3k&baseClass=ilrepositorygui) until the defined deadline. One Notebook per group is sufficient. Edit the teammember table below.\n",
    "\n",
    "**Important**: Also attach a HTML version of your notebook (```File```->```Download as```->```HTML```) in addition to the ```.ipynb```-File.\n",
    "\n",
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Geoffrey Hinton    |\n",
    "| 2.         | Yoshua Bengio      |\n",
    "| 3.         | Yann LeCun         |\n",
    "| 4.         | JÃ¼rgen Schmidhuber |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "Refresh the theory about graph neural networks from the lecture: https://maucher.pages.mi.hdm-stuttgart.de/mlbook/neuralnetworks/GraphNeuralNetworks.html. In this notebook, we will also use GNNs for Node Classification, but create an custom graph from spotify data. \n",
    "\n",
    "Questions:\n",
    "1. What are different prediciton tasks that can be solved with GNNs? \n",
    "2. Explain in a few sentences how the Graph Convolution layer works.\n",
    "3. Why do you need shortcut connections? \n",
    "4. Try to explain in which applications in general a GNN may be beneficial, compared to other neural network architecture-types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the genre of music tracks\n",
    "In this notebook we implement an own application of a Graph Neural Network for node classification. The GNN shall now predict the genre of a music-track (instead of the subject of the paper). Now, \n",
    "\n",
    "* our nodes are music-tracks, which are described initially only by their audio-features (instead of papers, which are described initially by their BoW). \n",
    "* we define two nodes to be connected, if the music-tracks are close together within a spotify-playlist (instead of nodes that are connected, if the paper of the one node has a reference to the paper of the other node). \n",
    "\n",
    "You do not have to implement your own GCN-layer as we will use the [Spektral Library](https://graphneural.network/).  However, the challenge now is to\n",
    "* access and preprocess the spotify-data\n",
    "* create a custom Spektral Dataset which can be used to train different GNNs with Spektral\n",
    "\n",
    "## Spotify Data\n",
    "Since the Spotify API has a rate limit and we also had issues with API changes last semester, we are making the JSON responses available offline for further use. To access the spotify data, the `SpotifyMock` class is provided, which returns the same JSON responses as the API. The data was collected on June 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "class SpotifyMock:\n",
    "\n",
    "    def __init__(self, root_path=\"./spotify_data\"):\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def load_json_for_path(self, path):\n",
    "        full_path = os.path.join(self.root_path, path)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "        \n",
    "    def categories(self):\n",
    "        return self.load_json_for_path('categories.json')\n",
    "\n",
    "    def category_playlists(self, category_id):\n",
    "        return self.load_json_for_path(f'categories/{category_id}.json')\n",
    "\n",
    "    def playlist_tracks(self, playlist_id):\n",
    "        return self.load_json_for_path(f'playlists/{playlist_id}.json')\n",
    "\n",
    "    def audio_features(self, track_id):\n",
    "        return self.load_json_for_path(f'tracks/{track_id}.json')\n",
    "\n",
    "spotify = SpotifyMock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data from spotify\n",
    "If you want to use the offical spotifiy API and want to do further experiments with other data, you need to apply for an [spotify developer account](https://developer.spotify.com/documentation/general/guides/app-settings/). Successful registration provides you a `client-ID` and a `client-secret`. In order to access data from spotify, we use the [spotipy Python package](https://spotipy.readthedocs.io/en/2.18.0/#). You can use the provided `client-ID` and a `client-secret` to connect to spotify. Once this connection has been established, all methods of [spotipy](https://spotipy.readthedocs.io/en/2.18.0/#) can be applied via this connector, e.g. `spotify.categories()`. The next two cells are of the type `raw`, so that they are not executed unintentionally."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install spotipy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import spotipy\n",
    "\n",
    "CLIENT_ID = \"\"\n",
    "CLIENT_SECRET = \"\"\n",
    "\n",
    "auth_manager = spotipy.oauth2.SpotifyClientCredentials(client_id=CLIENT_ID,\n",
    "                                        client_secret=CLIENT_SECRET)\n",
    "\n",
    "spotify = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "\n",
    "1. Apply the method `categories()` in order to receive the categories (genres) distinguished in spotify. **(Only for the official API: Note that in this and several other methods the value of the `limits`-argument must be increased in order to not only get the first elements.)**\n",
    "2. The categories relevant in this exercise are\n",
    "\n",
    "```CATEGORIES=[\"edm_dance\",\"romance\",\"metal\",\"classical\",\"jazz\",\"latin\",\"hiphop\"] ```\n",
    "\n",
    "3. Apply the method `category_playlists(category_id)` in order to get for each of the relevant categories a set of playlists (at least 60 playlists in sum).\n",
    "4. Apply the method `playlist_tracks(playlist_id)` in order to get for each of the playlists the music-tracks of the playlist. In this way you get for each track a list of features, such as artist, title, popularity, etc. \n",
    "5. Not contained in the set of features, returned by `playlist_tracks(playlist_id)` are the audio-features. However, these audio-features can be obtained by applying method `audio_features(track_id)`. \n",
    "6. Combine all the methods described above in order to create a pandas Dataframe which contains at least 4500 music tracks of the relevant categories. In the dataframe each track constitutes a row, and each row is described by the following features (columns):\n",
    "    \n",
    "    * unique-id of the track\n",
    "    * name of the playlist, that contains the track\n",
    "    * genre (category) of this playlist\n",
    "    * name of the track\n",
    "    * artist of the track\n",
    "    * position of the track within the playlist\n",
    "    * all the track's audio-features, which are returned by the method `audio_features(track_id)`.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Node Features and Edges\n",
    "\n",
    "In order to create the `graph_info` as in [the introductory example](#graphinfo), from the dataframe, created in the previous task\n",
    "1. Take the *audio-features* as `node_features` of the music tracks\n",
    "2. Create the `edges` array by connecting each music track to all tracks, which appear either in the N previous tracks or in the N successive tracks of the same playlist. (Set e.g. N=3).\n",
    "3. Initially you can set the weights of all edges to be 1. But maybe there is a better option?\n",
    "\n",
    "The `genre`-column of the dataframe, created in the previous task, constitutes the node-labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Node Features\n",
    "Inspect the `node_features` (audio-features of tracks). Is there need for some kind of preprocessing? Implement this preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv files\n",
    "\n",
    "After preprocessing the data, store the node features in a csv file and the edges in another csv file. These files can then be used for the custom dataset for the GNN, so we don't always have to create the graphs from the raw data.\n",
    "\n",
    "For reference, these are the final dataframes how your processed data should look: \n",
    "\n",
    "<img src=\"pics/spotify-data.png\" width=1800 />\n",
    "\n",
    "And the edges dataframe should look like this:\n",
    "\n",
    "<img src=\"pics/spotify-edges.png\" width=150 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the Baseline Model\n",
    "As in the [GNN-Lecutre](https://maucher.pages.mi.hdm-stuttgart.de/mlbook/neuralnetworks/GraphNeuralNetworks.html#build-a-baseline-neural-network-model) train and evaluate the baseline model. Adapt the hyperparameters of the baseline model to the current task. The necessary methods are already copied from the tutorial in the below cell.\n",
    "\n",
    "Create the training data only from the spotify data without the information about the edges, since we can not use this information for a standard neural network. \n",
    "\n",
    "`SparseCategoricalCrossentropy` can be used if you have the labels in integer format, if you have one-hot encoded labels, you need to use the `CategoricalCrossentropy`-function. The same is true for the metric `SparseCategoricalAccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def run_experiment(model, x_train, y_train):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def display_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history.history[\"loss\"])\n",
    "    ax1.plot(history.history[\"val_loss\"])\n",
    "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(history.history[\"acc\"])\n",
    "    ax2.plot(history.history[\"val_acc\"])\n",
    "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "\n",
    "def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=(num_features,), name=\"input_features\")\n",
    "    x = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block1\")(inputs)\n",
    "    for block_idx in range(4):\n",
    "        # Create an FFN block.\n",
    "        x1 = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block{block_idx + 2}\")(x)\n",
    "        # Add skip connection.\n",
    "        x = layers.Add(name=f\"skip_connection{block_idx + 2}\")([x, x1])\n",
    "    # Compute logits.\n",
    "    logits = layers.Dense(num_classes, name=\"logits\")(x)\n",
    "    # Create the model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits, name=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = create_baseline_model(hidden_units, num_classes, dropout_rate)\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_experiment(baseline_model, x_train, y_train)\n",
    "_, test_accuracy = baseline_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate a Graph Convolutional Network\n",
    "Now we train and evaluate a Graph Convolutional Network (GCN) with the help of the [Spektral Library](https://graphneural.network/). Let's get started by first reading the tutorial to understand the basic concepts of the framework: [Tutorial](https://graphneural.network/getting-started/) \n",
    "\n",
    "First we need to create a custom dataset and then we can try out different architectures. In the cell below, a skeleton of the class is provided, into which some methods must be implemented. As we represent songs as nodes and want to predict the genre of the song, we have one single big graph where we classify the nodes of the graph, therefore we need to create a dataset which can be used in the single mode. \n",
    "\n",
    "- Creating a Custom Dataset: [Tutorial](https://graphneural.network/creating-dataset/)\n",
    "    - Fill out the skeleton of `create_masks_for_splits`, where the indices of each data split are stored in the class attributes `mask_tr`, `mask_va` and `mask_te`, which are used later in the data loader. The data loader can then sample batches of the data only from the given indices (A reference implementation of the Cora Citation Dataset for Spektral is given [here](https://github.com/danielegrattarola/spektral/blob/master/spektral/datasets/citation.py)).\n",
    "    - Fill out the skeleton of `get_y`, where the labels are transformed into a one-hot encoded representation.\n",
    "    - Are more preprocessing steps necesseary so that the GNN converges? \n",
    "- Different Data modes: [Tutorial](https://graphneural.network/data-modes/)\n",
    "    - Explain in a few sentences, why we have different data modes for GNNs.\n",
    "- Train a GCN for predicting the music genre for the nodes in the graph\n",
    "    - Since our task is node prediction, similar to classifying the nodes of the Cora dataset, we can review the spectral example for node prediction: https://github.com/danielegrattarola/spektral/blob/master/examples/node_prediction/citation_gcn.py\n",
    "    - If you want to try different architectures, you can take a look at the examples of node-level predictions, which include some references for them: https://graphneural.network/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spektral.data import Dataset\n",
    "from spektral.data.graph import Graph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def _idx_to_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=bool)\n",
    "\n",
    "class SpotifyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset of five random graphs.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, edges_path, dtype=np.float32, **kwargs):\n",
    "        self.data_path = data_path\n",
    "        self.edges_path = edges_path\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.relevant_cols = ['intId', 'genre','danceability', 'tempo', 'acousticness',\n",
    "                             'duration_ms', 'energy', 'instrumentalness', 'liveness', \n",
    "                             'loudness','speechiness', 'time_signature', 'valence']\n",
    "        self.feature_names = list(set(relCols) - {\"intId\", \"genre\"})\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def download(self):\n",
    "        # Create the directory\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def create_masks_for_splits(self, audio_data, train_split=0.9):\n",
    "        \n",
    "        # index_tr = a list of indices of the training data in the whole dataset\n",
    "        # index_va = a list of indices of the validation data in the whole dataset\n",
    "        # index_te = a list of indices of the test data in the whole dataset\n",
    "        \n",
    "        # Train/valid/test masks\n",
    "        size_dataset = len(audio_data)\n",
    "        self.mask_tr = _idx_to_mask(index_tr, size_dataset)\n",
    "        self.mask_va = _idx_to_mask(index_va, size_dataset)\n",
    "        self.mask_te = _idx_to_mask(index_te, size_dataset)\n",
    "\n",
    "    def get_y(self, audio_data):\n",
    "        # return one hot encoded lables of the dataset\n",
    "        return\n",
    "        \n",
    "    def read(self):\n",
    "        df = pd.read_csv(self.data_path)\n",
    "\n",
    "        # the node features \n",
    "        x = df[self.feature_names].to_numpy()\n",
    "\n",
    "        # read the edges, we set the default edge weight to 1 for all edges\n",
    "        edges = pd.read_csv(self.edges_path)[[\"source\", \"target\"]]\n",
    "        edges['weight'] = 1\n",
    "        a = edges.to_numpy()\n",
    "\n",
    "        # create one-hot encoded labels\n",
    "        y = self.get_y(df)\n",
    "        \n",
    "        self.create_masks_for_splits(df)\n",
    "\n",
    "        # create adjacency matrix list fro edgelist     \n",
    "        G = nx.from_pandas_edgelist(edges)\n",
    "        a = nx.adjacency_matrix(G)\n",
    "        a.setdiag(0)\n",
    "        a.eliminate_zeros()\n",
    "\n",
    "        # Are more preprocessing steps necessary? \n",
    "\n",
    "\n",
    "        \n",
    "        return [\n",
    "            Graph(\n",
    "                x=x.astype(self.dtype),\n",
    "                a=a.astype(self.dtype),\n",
    "                y=y.astype(self.dtype),\n",
    "            )\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Discussion\n",
    "\n",
    "* Compare and discuss the results of the baseline- and the GCN-model. \n",
    "* Propose at least 3 other applications for which Graph Neural Networks may be suitable. For each of your proposals describe what the `node_features` and the `edges` may be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gnn_citations",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
