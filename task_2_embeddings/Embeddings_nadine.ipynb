{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Embeddings and Transformers\n",
    "Goals of this lecture are:\n",
    "* understand word embeddings\n",
    "* fine-tune a BERT model for text classification \n",
    "* visualize BERT embeddings\n",
    "* understand multimodal embeddings from CLIP \n",
    "* use CLIP for a image-to-text retrieval system\n",
    "* implement a text-to-text retrieval system with the universal sentence encoder model\n",
    "\n",
    "## Structure of this Notebook\n",
    "In this lecture, we begin by analyzing and understanding a pre-trained word2vec model that was used in earlier modern NLP systems before transformer models were released. We still use word2vec here to get a basic understanding of how words can be represented in neural networks and why transformer models have an advantage over word2vec embeddings.\n",
    "\n",
    "After analyzing the word2vec embeddings, we fine-tune a BERT model on the provided German news dataset and visualize the BERT embeddings to get a better understanding of the differences.\n",
    "\n",
    "The next major step after transformers for language modeling was the CLIP model, which combines text with images. We analyze the CLIP embeddings for different datasets and get an idea of why CLIP can be used for zero-shot classification of image data without fine-tuning the model on new data. We then use the CLIP model to find for given images, suitable news messages.\n",
    "\n",
    "In the last part of this lecture, we implement a text-to-text retrieval system by first understanding why CLIP is not well suited for text-to-text retrieval, and then using the universal sentence encoder model for retrieving text passages.\n",
    "\n",
    "This notebook will show you how easily these models can be used with the [HuggingFace](https://huggingface.co/) and [SentenceTransformers](https://www.sbert.net/) libraries, which provide a high-level interface to the aforementioned models. This lecture is primarily about understanding the concept of embeddings rather than how to define your own architecture and train it.\n",
    "\n",
    "First, however, we start by explaining the history and evolution of text representations for machine learning algorithms.\n",
    "\n",
    "Reuqired libraries:\n",
    "```\n",
    "conda install numpy scipy\n",
    "conda install scikit-learn\n",
    "conda install numba\n",
    "pip install umap-learn\n",
    "pip install plotly\n",
    "pip install 'transformers[torch]'\n",
    "pip install sentence-transformers\n",
    "pip install gensim\n",
    "````\n",
    "\n",
    "\n",
    "## Overall Picture and Evolution Path of Word Embeddings\n",
    "\n",
    "The conventional way of modelling documents in tasks like information-retrieval, document-clustering, document-classification, sentiment-analysis, topic-classification is to represent each document as a **Bag-Of-Word**-vector $$\\mathbf{d}_i=(tf_{i,0},tf_{i,1},\\ldots tf_{i,|V|}).$$ Each component of this vector corresponds to a single term $j$ of the underlying vocabulary $V$ and the values $tf_{i,j}$ counts the frequency of term $j$ in document $i$. Instead of the term-frequency $tf_{i,j}$ it is also possible to fill the BoW-vector with \n",
    "* a binary indicator which indicates if the term $j$ appears in document $i$\n",
    "* the tf-idf-values $$tfidf_{i,j}=tf_{i,j} \\cdot log \\frac{N}{df_j},$$ where $df_j$ is the frequency of documents, in which term $j$ appears, and $N$ is the total number of documents.\n",
    "\n",
    "Independent of the values used, the BoW model represents sets of documents as 2-dimensional numeric arrays, which can easily be passed to conventional Machine Learning algorithms, such as Logistic Regression, SVM, MLPs etc. However, the major drawbacks of this approach are:\n",
    "* the order by which terms appear in the document is totally ignored\n",
    "* semantic relatedness of terms is not modelled\n",
    "* BoW- vectors are very long and sparse \n",
    "\n",
    "As sketched in the picture below, today all of these drawbacks can be circumvented by representing the words of the document by their word-vectors and passing the corresponding sequence of word vectors in order to either a CNN or a Recurrent Neural Network such as LSTM or GRU. \n",
    "\n",
    "<img src=\"./Pics/overAllPicture.png\" style=\"width:600px\" align=\"middle\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Course of Action\n",
    "\n",
    "* Please write all executable python code in ```Code```-Cells (```Cell```->```Cell Type```->```Code```) and all Text as [Markdown](http://commonmark.org/help/) in ```Markdown```-Cells\n",
    "* Describe your thinking and your decisions (where appropriate) in an extra Markdown Cell or via Python comments\n",
    "* In general: discuss all your results and comment on them (are they good/bad/unexpected, could they be improved, how?, etc.). Furthermore, visualise your data (input and output).\n",
    "* Write a short general conclusion at the end of the notebook\n",
    "* Further experiments are encouraged. However, don't forget to comment on your reasoning.\n",
    "* Use a scientific approach for all experiments (i.e. develop a hypothesis or concrete question, make observations, evaluate results)\n",
    "\n",
    "## Submission\n",
    "\n",
    "Upload your complete Notebook to the Ilias course until the start of the next lecture. One Notebook per group is sufficient. Edit the teammember table below.\n",
    "\n",
    "**Important**: Also attach a HTML version of your notebook (```File```->```Download as```->```HTML```) in addition to the ```.ipynb```-File."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Teammember |                    |\n",
    "|------------|--------------------|\n",
    "| 1.         | Geoffrey Hinton    |\n",
    "| 2.         | Yoshua Bengio      |\n",
    "| 3.         | Yann LeCun         |\n",
    "| 4.         | Jürgen Schmidhuber |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tasks on Word Embeddings\n",
    "\n",
    "Word Embeddings for many languages can be downloaded for example from e.g. [FastText](https://fasttext.cc/docs/en/english-vectors.html). After downloading they can be imported into Python as described in [DSM.ipynb](DSM.ipynb). We use pretrained word2vec embeddings trained on [Common Crawl](https://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/). Please download the binary model for the German language from: https://fasttext.cc/docs/en/crawl-vectors.html#models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "model = load_facebook_model('/home/stud/n/nw091/mounted_home/pia2023/task_2_embeddings/Data_nadine/german_word_embeddings.bin.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain the meaning of `CBOW`, `dimension 300`, `n-grams of length 5`, `window of size 5` and `10 negatives` that were used for training of the provided word2vec models from this sentence:\n",
    "\n",
    "\"These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives.\" (adapted from https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanantion\n",
    "CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings\n",
    "\n",
    "#### Task 1: Meta information of the learned Word Embedding\n",
    "For the word2vec-model display the following parameters:\n",
    "* number of different words in the learned word2vec-model\n",
    "* length of the word2vec-vectors\n",
    "* context-length, applied in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Access Word Embeddings\n",
    "Display the first 10 components of the vectors of words\n",
    "* *hochschule*\n",
    "* *universität*\n",
    "* *anstalt*\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the words (not the word-vectors) at indices 0 to 49 of the Word-embedding. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Word Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Most similar words\n",
    "For a German word of your choice, display the 20 most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pair of German words of your choice, display their similarity-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two sets of German words of your choice, display their similarity-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Word Relations\n",
    "Apply the Word Embedding in order to answer the following questions:\n",
    "* *Mann is to König as Frau is to ?*\n",
    "* *Paris is to Frankreich as Berlin is to ?*\n",
    "* *Student is to Hochschule as Arbeiter is to ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Outliers\n",
    "Apply the Word Embedding in order to determine the outlier within the following set of words: *auto, motorrad, kran, essen*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Word Embeddings\n",
    "The following code-cell visualizes similarities of words from index 300 to 499 by transforming the high-dimensional word-vectors into a 2-dim space. The transformation is realized by [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://umap-learn.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import umap\n",
    "umap_model=umap.UMAP(n_components=2,random_state=0)\n",
    "model2d=umap_model.fit_transform(model.wv[model.wv.index_to_key[300:500]])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(19,14))\n",
    "idx=0\n",
    "for a in model2d[:300]:\n",
    "    #w=model.wv.index2word[300+idx].decode('utf-8')\n",
    "    w=model.wv.index_to_key[300+idx]\n",
    "    plt.plot(a[0],a[1],'r.')\n",
    "    plt.text(a[0],a[1],w)\n",
    "    idx+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Visualize similarities of selected target words and their neighbors\n",
    "For each of the word in the list `targetwords` calculate the 10 nearest neighbors. Then apply the same procedure as in the code cell above in order to visualize the similarities between all words in `wordset`, where `wordset` contains the target words and for each target word the 10 nearest neighbors. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Transformers for Text Classification\n",
    "\n",
    "In recent years Transformers have received increasing attention. Therefore we use the popular [Hugging Face Framework](https://huggingface.co/docs/transformers/model_doc/bert) to fine-tune a BERT model on our custom dataset.\n",
    "The classifier shall be able to distinguish RSS-feed news with technical content from general RSS-feed news. Training and evaluation data is available from the folder `./Data/GERMAN/`.\n",
    "\n",
    "Recap BERT Lecture: [MLBook](https://maucher.pages.mi.hdm-stuttgart.de/mlbook/transformer/attention.html#bert)\n",
    "\n",
    "Hugging Face provides thousands of different pretrained models for NLP, Visison or other tasks. They also have a [blog](https://huggingface.co/blog) and a [NLP Course](https://huggingface.co/course/chapter1/1) with Transformers.\n",
    "\n",
    "In this task we will use a the pretrained [Bert Model from deepset.ai](https://www.deepset.ai/german-bert) that was trained for hate speech detection on the GermEval18Coarse dataset and fine-tune it on our own classification task. Hugging Face provides a tutorial for a similar task in the [documentation](https://huggingface.co/transformers/v3.2.0/custom_datasets.html#sequence-classification-with-imdb-reviews). The model name used for loading the model from the library is: `deepset/bert-base-german-cased-hatespeech-GermEval18Coarse`\n",
    "\n",
    "First we need to install the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check text classification models <here: https://huggingface.co/models?filter=text-classification\n",
    "BERT_MODEL_NAME = \"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the environment variable `WANDB_DISABLED` to true so that Hugging Face is not automatically trying to connect to a Weigths & Biases account. \n",
    "\n",
    "[Weights & Biases](https://wandb.ai/) is a machine learning platform for developers to build models faster. It provides tools to track experiments and datasets, evaluate model performance, reproduce models, visualize results etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access locally stored RSS-feed news of both categories \n",
    "News crawled from RSS feeds of category `Tech` and `General` are stored in distinct directories. In the following code-cell the paths to both directories are configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.092680Z",
     "start_time": "2018-02-28T15:30:19.088067Z"
    }
   },
   "outputs": [],
   "source": [
    "techpath=\"./Data/GERMAN/TECH/RSS/FeedText\"\n",
    "generalpath=\"./Data/GERMAN/GENERAL/RSS/FeedText\"\n",
    "catpaths=[techpath,generalpath]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specified directory `techpath` must contain a list of directories. The name of each of these directories is the name of the feed, from which the news are crawled attached by the date of crawling. For example one subdirectory may be `golem-2017-11-07`. Each of this feed-and-date-specific subdirectories conatins one or more `.txt`-files. The name of each of these `.txt`-files is of format `hh-mm.txt`, i.e. it contains the daytime of crawling. If one feed is crawled multiple times per day, the feed-and-date-specific subdirectories will contain more than one `.txt`-files, each identified by the time of crawling. Below, the first lines of a sample `.txt`-file, `15-15.txt` from directory `golem-2017-11-07` are shown. Each paragraph of the file is a single news item from the *golem RSS feed*, crawled at 2017-11-07, 15.51h.\n",
    "\n",
    "![Screenshot of feedfile](./Pics/feedfile.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code-cell all news from all files are imported. All news from subdirectories of `techpath` are assigned to the class `Tech` (label 0) and all news from subdirectories of `generalpath` are assigned to class `GENERAL` (label 1). Each news-item is represented as a list of words, contained in the item. All item-specific word-lists are assigned to the list `texts` and the corresponding class-indices are assigned to the list `labels`. Before news-item specific word lists are appended to the `texts`-list it is checked, if they are not yet contained in this list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.366956Z",
     "start_time": "2018-02-28T15:30:19.097007Z"
    }
   },
   "outputs": [],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for catlabel,categorypath in enumerate(catpaths):\n",
    "    for name in sorted(os.listdir(categorypath)):\n",
    "        feedpath = os.path.join(categorypath, name)\n",
    "        if os.path.isdir(feedpath):\n",
    "            #print(name)\n",
    "            for fname in sorted(os.listdir(feedpath)):\n",
    "                fpath = os.path.join(feedpath, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='utf-8')\n",
    "                t = f.read()\n",
    "                news=t.split('\\n \\n')\n",
    "                for entry in news:\n",
    "                    if (len(entry)>50) and (entry not in texts): #remove duplicates\n",
    "                        #if entry not in texts:\n",
    "                        texts.append(entry)\n",
    "                        labels.append(catlabel)\n",
    "                f.close()\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.389732Z",
     "start_time": "2018-02-28T15:30:19.374554Z"
    }
   },
   "outputs": [],
   "source": [
    "print(texts[0])\n",
    "print(\"-\"*20)\n",
    "print(texts[1])\n",
    "print(\"-\"*20)\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Visualize distribution of document lengths\n",
    "Visualize the distribution of the number of words in all documents in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_of_words = []\n",
    "\n",
    "for text in texts:\n",
    "    num_of_words.append(len(text))\n",
    "    \n",
    "# print(num_of_words)\n",
    "\n",
    "# Number of words - document\n",
    "plt.hist(num_of_words, bins=range(0,10))\n",
    "plt.xlabel(\"Label of Text\")\n",
    "plt.ylabel(\"Number of words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Load the Data into a custom Dataset  \n",
    "\n",
    "#### Convert texts to sequence of integers\n",
    "\n",
    "Apply the [BertTokenizer](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertTokenizer) in order to uniquely map each word to an integer-index and to represent each text (news-item) as a sequence of integers. For more information, see the [Tokenizer-chapter](https://huggingface.co/course/chapter2/4?fw=pt) in the NLP course. Afterwards we load the custom texts into a custom Dataset. Also define the `max_length` of the sequences and use padding and truncation. We first need to split the dataset into a train and test split and then apply the tokenizer on the splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a custom Dataset can be created for training of the model. We will use a PyTorch Dataset here, as this model was trained with PyTorch and also to show that using the library with PyTorch is quite simple even though we have not used PyTorch in this course. In addition more PyTorch models are hosted on hugging face than Tensorflow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Define the Bert Model\n",
    "Next we can create a [Bert Model for Sequence Classification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification) with the pretrained weights from the previously definded `BERT_MODEL_NAME`. We can use GPU support for training the model by calling `model = model.to('cuda')` after loading the model. This is, however, not necessary as the dataset is quite small and we also do not need to train the model for many epochs. \n",
    "\n",
    "For training the Bert-Model on a custom task we also need to set the custom number of the labels and also the flag `ignore_mismatched_sizes` to true. The parameters which can be set are listed in the [PretrainedConfig](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Define Training Arguments and train the Model\n",
    "Hyperparameters for training a Hugging face Transformer Model are defined in a [TrainingArguments](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments)-Object. We can mostly use the default arguments, but choose an appropriate batch size. Two epochs is already enough for finetuning of the model.\n",
    "\n",
    "Additonally, we set the `evaluation_strategy`and `save_strategy` to `epoch` to make the model predicitons comparable with the CNN and LSTM-architecture. You also need to define a suitable value for `logging_steps`. This value should be smaller than the steps needed to complete an epoch as otherwise no values will be logged in the first epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a [Trainer](https://huggingface.co/transformers/v3.2.0/main_classes/trainer.html#transformers.Trainer) with the defined model, training arguments and both datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Add an Evaluation Metric while Training the Model\n",
    "As you should have already noticed, by default Hugging Face returns only loss and no other metrics. To get metrics for our training epochs, we should set the `compute_metrics` parameter of the Trainer. Define a function with the approriate metrics so that you can compare your metrics to the previous models. An example of a compute metrics function can be found [here](https://huggingface.co/transformers/v3.0.2/training.html#trainer).\n",
    "\n",
    "Then retrain your model and interpret the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Visualize BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Bert for Inference and compare example Sentences\n",
    "After the model is trained, we can visualize the embeddings for the dataset and use the model for inference. For getting the hidden states of the model prediciton, you have to pass `output_hidden_states=True` to the forward/call method of the model. Additionally, you can pass `return_dict=True` to get a [BaseModelOutput](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)-object, where you can then access the `last_hidden_state`-attribute.  \n",
    "\n",
    "To understand the difference of the embeddings, you can use two example sentences where one word in the middle differs and compare the embeddings of both sentences. \n",
    "For comparing the two example sentences use `torch.isclose(first_sentence, second_sentence, atol=1e-05)` [Doc](https://pytorch.org/docs/stable/generated/torch.isclose.html). The `atol` parameter controls which numerical precision satsifies the `isclose` condition.\n",
    "\n",
    "What can you observe in contrast to the word2vec embeddings? What is the main difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Create Embeddings for whole Dataset\n",
    "\n",
    "Now we can create the embeddings for the entire dataset. Store the embeddings in a list so that we can visualize the embedding space. We need to use a dimensional reduction method like UMAP, which was used previously to visualize the word2vec embeddings. Average the embeddings over the first dimension so that we get a 2-dimensional tensor with `(number of texts, size of bert embedding)`.\n",
    "\n",
    "For visualization, we can use a [Plotly scatter plot](https://plotly.com/python/line-and-scatter/). To analyze which sentences are close to each other in the embedding space, set the news text as hover data. Display only the first 100 characters of the news article, otherwise the hover box will be to large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Visualize CLIP embeddings\n",
    "\n",
    "This part was inpsired by: https://medium.com/mlearning-ai/having-fun-with-clip-features-part-i-29dff92bbbcd\n",
    "\n",
    "After covering the differences of the transformer and word2vec embeddings, we will analyze the multimodal embeddings generated from the [OpenAI CLIP](https://openai.com/research/clip). Unlike GPT-4 the CLIP model was still released open source with pretrainend weights. We will use the pretrained weights, so we do not need to finetune the model. \n",
    "\n",
    "The main goal of this chapter is to understand why CLIP can be used for zero-shot classification on image datasets. Therefore, we analyze two different datasets, [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) and the good old [MNIST](https://paperswithcode.com/dataset/mnist) dataset. \n",
    "\n",
    "\n",
    "## CLIP Overview\n",
    "CLIP, Contrastive Language-Image Pre-training, was trained on over 400M image and text pairs collected from publicly available resources on the internet. A text-image pair consists of the image and a caption. You can think of the caption as the description of the image, such as when someone uploads a picture to Instagram and briefly describes what she/he is doing. \n",
    "\n",
    "<img src=\"Pics/clip-training.svg\" width=500 />\n",
    "The figure above shows an overview of the training objective. During training, CLIP is incentivized to match each image in the current batch to the correct caption. Then the loss for both encoders is calculated and backpropagated. This pushes the representation ob both encoders onto a common latent space. All other examples in the current batch are negative examples, as the caption doesn't fit to the image. This is called contrastive learning. The goal of contrastive learning is to learn representations in which the embeddings of similar instances are close to each other, while different instances are further away. \n",
    "\n",
    "For using CLIP in a zero-shot way to classify datasets, we generate text prompts for all classes in the style of `a photo of a {class_name}`. Then embeddings are created for all images and text prompts. To classify which image belongs to which class, the cosine similarity between the text prompts and image embeddings is calculated and the text prompt with the highest similarity is selected. See also the following image:\n",
    "<img src=\"Pics/clip-overview.svg\" width=500 />\n",
    "(both pictures from https://openai.com/research/clip)\n",
    "\n",
    "## Projection onto the same embedding space\n",
    "We will use UMAP again for dimensional reduction. Unlike the visualization of the BERT and word2vec embeddings, CLIP consists of two different encoders, one for the image and one for the text. UMAP is able to map the embedding spaces of both encoders onto the same embedding space by first fitting and transforming the embedding space of the image dataset, and then using the fitted UMAP model to map the text embedding space onto the image embedding space. \n",
    "\n",
    "We will provide a function for plotting the text prompts into the embedding space, as this is a bit more complicated, to get matching colors for the text prompts and the labels of the images. It is also necessary to sort the image embeddings by the labels, before plotting the data. This ensures, that we can match the colors of the text prompts and the images. \n",
    "\n",
    "From now on we will also use the [SentenceTransformers](https://www.sbert.net/) library, which is another abstraction of the HuggingFace library and thus also easier to use. For further information on how to use CLIP with the SentenceTransformer library, see the [docs](https://www.sbert.net/examples/applications/image-search/README.html).\n",
    "\n",
    "The CLIP model can be loaded with the name `clip-ViT-B-32`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/python3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-20 10:55:19.953463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 10:55:21.442222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load CLIP model (Non-Multilingual)\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# There is a warning as output, but it works anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mnist (/home/stud/n/nw091/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n",
      "100%|██████████| 2/2 [00:00<00:00, 207.38it/s]\n",
      "Found cached dataset cifar10 (/home/stud/n/nw091/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 305.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Installing huggigface datasets and adjusting to warnings\n",
    "# !pip install datasets\n",
    "# !pip install --force-reinstall charset-normalizer==3.1.0\n",
    "\n",
    "# Load Embeddings from HuggingFace\n",
    "from datasets import load_dataset\n",
    "dataset_mnist = load_dataset(\"mnist\")\n",
    "dataset_cifar10 = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dataset_cifar10.__dict__) -> Output: {}\n",
    "\n",
    "dataset_cifar10['train'][:1280]\n",
    "print(len(dataset_cifar10))\n",
    "print(dataset_cifar10)\n",
    "\n",
    "# Ouput of first image\n",
    "dataset_cifar10['train'][0]['label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert label number to string\n",
    "dataset_cifar10['train'][0]['label']\n",
    "\n",
    "dataset_cifar10['train'].features['label'].names\n",
    "\n",
    "class\n",
    "# tokenized_batch[\"labels\"] = [str_to_int[label] for label in batch[\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import umap\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "This function plots the low dimensional text prompts embeddings \n",
    "onto the same figure with a x marker to distinguish them from the image embeddings\n",
    "\n",
    "Parameters:\n",
    "    low_dim_text_embeddings: the 2-dimensional text prompt embeddings after the UMAP transform\n",
    "    figure: the plotly figure, returned by the px.scatter function\n",
    "    classes: the sorted classes of the dataset\n",
    "\"\"\"\n",
    "def plot_text_prompt_embeddings(low_dim_text_embeddings, figure, classes):\n",
    "    l, c = len(px.colors.qualitative.Plotly), len(low_dim_text_embeddings)\n",
    "    df, colors = pd.DataFrame(low_dim_text_embeddings), (px.colors.qualitative.Plotly * ((c // l) + 1))[:c]\n",
    "    figure.add_scatter(x=df[0], y=df[1], hovertext=classes, mode='markers', marker={'symbol':\n",
    "                   'x', 'size':20, 'color': colors, 'opacity':0.8, 'line': dict(width=2, color=\"DarkSlateGrey\")})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function should load a dataset with a given name from the HuggingFace datasets\n",
    "and create the embeddings for the images and class labels.\n",
    "You need to create the automatic text prompts in the style of \"a photo of a {class_name}\" for each class.\n",
    "\n",
    "It is also sufficient to plot a large enough subset of the data. \n",
    "n=1280 is already good enough to see, how the embedding space is clustered.\n",
    "It could be that the keys in which the images are stored are not always the same.\n",
    "\n",
    "Parameters:\n",
    "    name: the name of the dataset, e.g. 'cifar10' or 'mnist'\n",
    "    n: the number of images for which embeddings should be created.\n",
    "    \n",
    "Return: image embeddings, text embeddings, labels \n",
    "\"\"\"\n",
    "def load_embeddings_for_dataset(name, n=1280):c\n",
    "    # Load Embeddings from HuggingFace\n",
    "    from datasets import load_dataset\n",
    "    dataset_mnist = load_dataset(\"mnist\")\n",
    "    dataset_cifar10 = load_dataset(\"cifar10\")\n",
    "    \n",
    "    dataset_cifar10.__dict__\n",
    "    \n",
    "    # check if the embeddings were already created\n",
    "    if os.path.exists('img_emb_file') and os.path.exists('text_emb_file'):\n",
    "        pass\n",
    "         \n",
    "    else:\n",
    "        # create new embeddings for the text prompts and images\n",
    "        use_precomputed_embeddings = True\n",
    "        \n",
    "        # if use_precomputed_embeddings:\n",
    "            # class_name = '\n",
    "        print(\"a photo of a {class_name}\")\n",
    "        \n",
    "        \n",
    "        # Embeddings in zwei Schritten\n",
    "        # TODO embeddings = model.encode(sentences)\n",
    "\n",
    "        # TODO\n",
    "        img_names = list(glob.glob('photos/*.jpg'))\n",
    "        print(\"Images:\", len(img_names))\n",
    "        img_emb = img_model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "        # and store them on the disk so that we not always need to calculate the embeddings\n",
    "        pass\n",
    "        \n",
    "    return # image embeddings, text embeddings, labels \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function transforms the data so that we can plot them. \n",
    "First, use UMAP to reduce the embeddings into 2 dimensions.\n",
    "Then use px.scatter to plot the image embeddings, use the class labels as the color of the data point.\n",
    "\n",
    "Parameters:\n",
    "    img_embeddings: the image embeddings from the CLIP model\n",
    "    text_embeddings: the text embeddings from teh CLIP model\n",
    "    labels: the labels for the images\n",
    "\"\"\"\n",
    "def plot_embeddings(img_embeddings, text_embeddings, labels):\n",
    "    # use UMAP to reduce the dimensionality of the image data\n",
    "    # ToDo: So ähnlich:\n",
    "    # umap_model=umap.UMAP(n_components=2,random_state=0)\n",
    "    # model2d=umap_model.fit_transform(model.wv[model.wv.index_to_key[300:500]])\n",
    "    \n",
    "    # plot the low dim data\n",
    "\n",
    "    #plot_text_prompt_embeddings(low_dim_text_embeddings, fig, classes)\n",
    "\n",
    "    #fig.show()\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: CIFAR10 Dataset\n",
    "\n",
    "The evaluation of the CIFAR10 dataset. Can you draw some conclusions about CLIP and the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><b>Conclusion:</b></font>\n",
    "CLIP seems like an efficient way of learning deep connections between image representation and matching text. The contrastive learning approach therefore is approachable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: MNIST dataset\n",
    "The evaluation of the MNIST dataset. Can you draw some conclusions about CLIP and the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><b>Conclusion:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Not as good as CIFAR10\n",
    "    - Latent space is clustered (single cluster for each class)\n",
    "- It seems like the pairs that the black and white images of MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Custom Dataset\n",
    "\n",
    "After analyzing the CIFAR10 and MNIST dataset, perform the same evaluation on a dataset of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR-100? --> 100 classes instead of 10; number suffix refers to the number of classes / categories\n",
    "Fashion MNIST: still only 10 labels. The more labels, the more\n",
    "Imagenet\n",
    "PASCAL\n",
    "VOC\n",
    "COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Text and Image Retrieval System\n",
    "\n",
    "In this section, we will use CLIP to match images to news articles. Therefore, we need to create text embeddigs for the whole dataset. As we are working with German news articles, we have to use the multilingual version of CLIP with the model name `clip-ViT-B-32-multilingual-v1` for encoding the text. For the images, we still use the same model as before `clip-ViT-B-32`. For this notebook, it is not necessary to understand the details of how the CLIP text encoder was adapted to more languages, but if this topic is of interest, you can read the paper here: [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813)\n",
    "\n",
    "As an side experiment, we will also try to use CLIP for text-to-text retrieval and check if it is possible to use CLIP for zero-shot text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create embeddings for all news articels with the CLIP text encoder\n",
    "\n",
    "The maximum sequence length of the CLIP text encoder is unfortunately not very long, but we can simply truncate the news article to the first 170 characters so that the text encoder can create embeddings for all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the multilingual CLIP model. Note, this model can only encode text.\n",
    "# If you need embeddings for images, you must load the 'clip-ViT-B-32' model\n",
    "text_encoder = SentenceTransformer('clip-ViT-B-32-multilingual-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Match News Articles to Images\n",
    "\n",
    "Define a function `show_news_for_img(img_path, topk=5)`, which takes an image path as input and an integer `topk` that defines how many of the best matching news articles should be displayed. For evaluation, you can select some example images (minimum 5) from [Unsplash](https://unsplash.com/) and analyze how good the best 5 news articles match to the image.\n",
    "\n",
    "The function should always display the image and afterwards the topk news articles and the similarity of the news article and image.\n",
    "\n",
    "For calculating which news article fits best to an image, use the [cosine similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) metric provided from Scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: CLIP for zero-shot text classification\n",
    "\n",
    "In this experiment, analyze how good CLIP is for zero-shot text classification.\n",
    "Use `class_prompts = ['Dies ist ein Technischer Text', 'Dies ist ein Genereller Text']` as the zero-shot class text prompts to assign the news articles to one of the two categories.\n",
    "\n",
    "Then calculate the cosine similarity between the news articles and the text and check which class has the higher similarity. Calculate the accuracy of the zero-shot text classifier. How do you interpret the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: CLIP for text-to-text retrieval\n",
    "\n",
    "Define a function `show_news_for_keywords(keywords, topk=5)`, which takes a string as input and an integer topk that defines how many of the best matching news articles should be displayed. For evaluation, try out some different keywords such as `Sport Fußball` or `Donald Trump` and interpret the best matching news articles. What final conclusions can you draw about the CLIP model? Why do you think CLIP shows this kind of behavior? \n",
    "\n",
    "The function should always display the keywords and then the top news articles, as well as the similarity of the news article to the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Image-to-image retrieval with CLIP\n",
    "\n",
    "As an bonus experiment you can test, how good CLIP is for image-to-image retrieval. This should be straightforward to implement, since you already know, how to use clip for image-to-text and text-to-text retrieval. \n",
    "\n",
    "What hypothesis do you have regarding the performance of CLIP's image-to-image capabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Semantic Search with only text-based model\n",
    "\n",
    "In the final experiment, we will use the [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html) model for text-to-text retrieval. \n",
    "The model name is `distiluse-base-multilingual-cased-v1`.\n",
    "\n",
    "CLIP uses one image and one text encoder to link text and images. In contrast this model uses a shared text encoder as it only computes embeddings for text passages. Instead of image-text pairs, we have text-text pairs, where one text is a paragraph (similar to the image) and the other text is similar to the caption. The model is trained in a multi-task setting. The following picture shows the training objectives for each task. All tasks receive two text segments as input and in addition, the question answering and translation tasks have almost the same objective as CLIP, which is to match one text block to another text block. \n",
    "\n",
    "<img src=\"Pics/use-model.png\" width=500 />\n",
    "(img src: https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html)\n",
    "\n",
    "Recalculate the text embeddings for all news articles with the new model. Then perform the same experiments as in part 5 task 3 and task 4. It should be possible to use the same method `show_news_for_keywords(keywords, topk=5)` as in the previous experiment.\n",
    "\n",
    "Interpret the accuracy of the zero-shot text classification experiment compared to the experiment before? \n",
    "How good is the news article retrieval system for the same keywords as in part 5 task 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further ideas\n",
    "\n",
    "#### This is not part of the official notebook!!! \n",
    "\n",
    "This was part of the old notebook, where LSTM and CNN models were trained on top of the word2vec-embeddings for text classification and can be used for further experiments/ideas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks on Text Classification\n",
    "\n",
    "In this section a CNN document classifier shall be defined, trained and evaluated with Keras. For this the basics of Keras, as described e.g. in this [Keras Tutorial](https://keras.io/about/), must be known. \n",
    "\n",
    "The classifier shall be able to distinguish RSS-feed news with technical content from general RSS-feed news. Training and evaluation data is available from the folder `./Data/GERMAN/`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 16:17:17.112412: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 16:17:17.685486: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 16:17:17.687403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 16:17:19.708968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.12.0 | Keras version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(f\"Tensorflow version: {tf.__version__} | Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a test file\n",
    "#df = pd.read_csv('Data/GERMAN/TECH/RSS/FeedText/computerbild-2017-09-29/12-13.txt')\n",
    "#print(df.shape)\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access locally stored RSS-feed news of both categories \n",
    "News crawled from RSS feeds of category `Tech` and `General` are stored in distinct directories. In the following code-cell the paths to both directories are configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.092680Z",
     "start_time": "2018-02-28T15:30:19.088067Z"
    }
   },
   "outputs": [],
   "source": [
    "techpath=\"./Data/GERMAN/TECH/RSS/FeedText\"\n",
    "generalpath=\"./Data/GERMAN/GENERAL/RSS/FeedText\"\n",
    "catpaths=[techpath,generalpath]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specified directory `techpath` must contain a list of directories. The name of each of these directories is the name of the feed, from which the news are crawled attached by the date of crawling. For example one subdirectory may be `golem-2017-11-07`. Each of this feed-and-date-specific subdirectories contains one or more `.txt`-files. The name of each of these `.txt`-files is of format `hh-mm.txt`, i.e. it contains the daytime of crawling. If one feed is crawled multiple times per day, the feed-and-date-specific subdirectories will contain more than one `.txt`-files, each identified by the time of crawling. Below, the first lines of a sample `.txt`-file, `15-15.txt` from directory `golem-2017-11-07` are shown. Each paragraph of the file is a single news item from the *golem RSS feed*, crawled at 2017-11-07, 15.51h.\n",
    "\n",
    "![Screenshot of feedfile](./Pics/feedfile.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code-cell all news from all files are imported. All news from subdirectories of `techpath` are assigned to the class `Tech` (label 0) and all news from subdirectories of `generalpath` are assigned to class `GENERAL` (label 1). Each news-item is represented as a list of words, contained in the item. All item-specific word-lists are assigned to the list `texts` and the corresponding class-indices are assigned to the list `labels`. Before news-item specific word lists are appended to the `texts`-list it is checked, if they are not yet contained in this list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.366956Z",
     "start_time": "2018-02-28T15:30:19.097007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 3315 texts.\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for catlabel,categorypath in enumerate(catpaths):\n",
    "    for name in sorted(os.listdir(categorypath)):\n",
    "        feedpath = os.path.join(categorypath, name)\n",
    "        if os.path.isdir(feedpath):\n",
    "            #print(name)\n",
    "            for fname in sorted(os.listdir(feedpath)):\n",
    "                fpath = os.path.join(feedpath, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='utf-8')\n",
    "                t = f.read()\n",
    "                news=t.split('\\n \\n')\n",
    "                for entry in news:\n",
    "                    if (len(entry)>50) and (entry not in texts): #remove duplicates\n",
    "                        #if entry not in texts:\n",
    "                        texts.append(entry)\n",
    "                        labels.append(catlabel)\n",
    "                f.close()\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.389732Z",
     "start_time": "2018-02-28T15:30:19.374554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestaltungstipps: T-Shirts bedrucken\n",
      "Bedruckte T-Shirts sind ein All-Time-Favorite – egal, ob witzige Comic-Motive oder schlichte Formen. Tipps und Ideen rund um die Gestaltung.\n",
      "--------------------\n",
      "Software-Charts: Die 50 Top-Downloads des Monats\n",
      "CCleaner ist gehackt, die sichere neue Fassung laden Sie hier – samt Vollversionen: das Ashampoo Music Studio und PowerDirector.\n",
      "--------------------\n",
      "Quereinsteiger-Jobs: So gelingt der Wechsel\n",
      "Der Quereinstieg in eine neue Branche ist heute nicht mehr ungewöhnlich. Fragen und Tipps, die beim beruflichen Neustart helfen können.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(\"-\"*20)\n",
    "print(texts[1])\n",
    "print(\"-\"*20)\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert texts to sequence of integers\n",
    "\n",
    "#### Task 10: Transform each text into a sequence of integers\n",
    "Apply the [Keras Tokenizer class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) in order to uniquely map each word to an integer-index and to represent each text (news-item) as a sequence of integers. The maximum number of words regarded in the Tokenizer shall be `MAX_NB_WORDS=10000`. After fitting the `Tokenizer`-object with the available texts (`fit_on_texts()`), it's attribute `tokenizer.word_index` maps each word to an integer-index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:19.893962Z",
     "start_time": "2018-02-28T15:30:19.393772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20652 mappings of words to an integer index.\n",
      "The word 'und' can be found on index 3.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "indicies = word_tokenizer.word_index\n",
    "\n",
    "# Examples\n",
    "print(f\"There are {len(indicies)} mappings of words to an integer index.\")\n",
    "# first10values = {k: indicies[k] for k in sorted(indicies.keys())[:20]}\n",
    "print(f\"The word 'und' can be found on index {indicies['und']}.\")\n",
    "print(indicies.get('und'))\n",
    "\n",
    "# This is wrong, this should list the key-value pairs. What did I do wrong?\n",
    "for key, value in indicies.items():\n",
    "    if value == 'und':\n",
    "        print(key)   \n",
    "\n",
    "#print(indicies)\n",
    "\n",
    "# Checking the created tokens\n",
    "# for i, token in enumerate(indicies[10][:10]):\n",
    "    # print(token, \"=\", tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Visualize distribution of document lengths\n",
    "Visualize the distribution of document lengths in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "lengths = [len(text.replace(\"-\", \"\").split(\" \")) for text in texts]\n",
    "plt.hist(lengths, bins=20, color=\"purple\")\n",
    "plt.title(\"Word counts of texts\")\n",
    "plt.xlabel(\"Word count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Pad sequences to unique length\n",
    "Next the [Keras function pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) shall be applied to represent the set of all sequences as a numpy-array. The number of rows in this array is the number of sequences, the number of columns is the fixed sequence length `maxlen`. Choose an appropriate value for the fixed sequence length. \n",
    "\n",
    "Moreover, the labels-list shall be one-hot-encoded into a numpy-array `labels` with two columns. In this array the entry in row i, column j is 1, if the i.th text belongs to the class with label j, otherwise this entry is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3315"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:20.236503Z",
     "start_time": "2018-02-28T15:30:20.180082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   43,    1, 8919],\n",
       "       [   0,    0,    0, ..., 2720,    3, 8925],\n",
       "       [   0,    0,    0, ..., 4137,  601,   76],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    7,  310, 2507],\n",
       "       [   0,    0,    0, ...,   53,   88,   52],\n",
       "       [   0,    0,    0, ..., 1513,   12,  138]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.keras.utils.pad_sequences(indicies, maxlen=1000)\n",
    "\n",
    "sequences = word_tokenizer.texts_to_sequences(texts)\n",
    "tf.keras.utils.pad_sequences(sequences, maxlen=60)\n",
    "\n",
    "# ToDo: Zweiter Satz verstehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><b>Comment:</b></font> For the value of fixed sequence length a maxlength of 60 was chosen, as around 90% of texts ha size that long. Because most of these texts have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split labeled texts in training- and validation set\n",
    "\n",
    "#### Task 13: Create training- and validation-partition\n",
    "The numpy-array of padded integer-sequences and the corresponding labels (as generated in the code cell above) shall be randomly shuffled and then split into a training- and a validation set. 80% of the labeled data is applied for training, the remaining 20% for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:30:28.636659Z",
     "start_time": "2018-02-28T15:30:28.625877Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  and prepare Word-Embedding\n",
    "\n",
    "#### Task 14: Load Word Embedding\n",
    "Load the Word Embedding, which has been trained and saved in task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:31:13.127470Z",
     "start_time": "2018-02-28T15:30:58.634797Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Prepare `embeddings_index`\n",
    "Next a Python dictionary `embeddings_index`, which maps words to their vector-representation must be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:42:20.512927Z",
     "start_time": "2018-02-28T15:42:20.252855Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 16:  Prepare `embeddings_matrix`\n",
    "\n",
    "The variable `word_index` has been allocated in **Task 10**. It maps each word of the labeled documents (news) to a unique index. Now `word_index` as calculated from the labeled input documents for the classifier is combined with the `embedding_index`, which maps each word of the word-embedding (German wikipedia dump in this case) to it's vector representation. The result is the *numpy-array* `embedding_matrix`, which contains in the i.th row the vector representation of the word with index i in the `word_index`. Applying this `embedding_matrix` allows the *Keras*-module `Embedding` to map a sequence of word-indices, which represents a single news-item, to a sequence of word-vectors. \n",
    "\n",
    "![wordMappings](./Pics/wordMappings.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:42:22.912060Z",
     "start_time": "2018-02-28T15:42:22.859162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN architecture, train and evaluate\n",
    "In Keras network architectures can either be defined as [Sequential models](https://keras.io/guides/sequential_model/) or by the [Functional API approach](https://keras.io/guides/functional_api/). For the implementations in this lecture you are free to choose any of both. \n",
    "\n",
    "#### Task 17: Define CNN architecture\n",
    "The `embedding_matrix`, as generated in the previous task shall now be assigned to the `weights`-argument of a [Keras Embedding Layer](https://keras.io/api/layers/core_layers/embedding/). The Embedding Layer constitutes the input layer of the CNN. \n",
    "\n",
    "Following the input layer configure one or more cascades of `Conv1D`-layers, followed by `MaxPooling1D`-layers. Apply `relu`-activation in the conv-layers. \n",
    "\n",
    "\n",
    "After the last convolutional-layer a `GlobalMaxPooling1D`-layer shall be applied, followed by a dense layer with softmax-activation (for the binary classification actually a sigmoid-activation at the output would be enough).\n",
    "\n",
    "Display a summary of this architecture by calling the `summary()`-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:42:28.130765Z",
     "start_time": "2018-02-28T15:42:27.256479Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 18: Define Training\n",
    "Hyperparameters for training a Keras model are defined by applying the `compile()`-method. Use `binary_crossentropy` as loss-function, `rmsprop` as learning-algorithm (optimizer) and `accuracy` as metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T15:49:12.003106Z",
     "start_time": "2018-02-28T15:49:11.940070Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 19:  Training and Validation\n",
    "Apply the model's `fit()`-method for training. Assign training- and validation- data as generated in Task 13. Choose an appropriate `batch_size` and an appropriate number of `epochs`. \n",
    "\n",
    "After training is finished, plot the accuracy on the training- and validation-data over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "#### Task 20: Optimize CNN Architecture\n",
    "Find a good CNN configuration by varying hyperparameters, such as number of layers, number of filters, filtersizes, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further ideas\n",
    "If no embedding_matrix is assigned to the `weights`-argument of the `EmbeddingLayer` and the `trainable`-argument is set to be `True`, then the weights of the EmbeddingLayer are learned during training of the entire network. I.e. in this case no pre-trained word-embedding is required. It would be nice to compare this approach with the approach where pre-trained weights are applied. \n",
    "\n",
    "As an alternative to the CNN a LSTM-architecture, e.g. a single LSTM-Layer, followed by Dropout and a Dense Layer at the output, can be applied for classification. The input-embedding layer would be the same as for the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "899px",
   "left": "0px",
   "right": "1789.23px",
   "top": "153.883px",
   "width": "357px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
